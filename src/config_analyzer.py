"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Cline
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –≤—ã—è–≤–ª—è–µ—Ç –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
import time

class ConfigAnalyzer:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def analyze_current_setup(self, cline_config_path: Path, workspace_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Cline"""
        config_data = {
            'timestamp': time.time(),
            'workspace_path': str(workspace_path),
            'cline_config_path': str(cline_config_path),
            'current_rules': self._analyze_clinerules(workspace_path),
            'current_workflows': self._analyze_workflows(workspace_path),
            'vscode_settings': self._analyze_vscode_settings(cline_config_path.parent.parent),
            'project_structure': self._analyze_project_structure(workspace_path),
            'usage_patterns': self._analyze_usage_patterns(cline_config_path),
            'performance_issues': self._identify_performance_issues(),
            'optimization_opportunities': []
        }
        
        # –í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        config_data['optimization_opportunities'] = self._identify_optimization_opportunities(config_data)
        
        self.logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(config_data['optimization_opportunities'])} –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è")
        
        return config_data
        
    def _analyze_clinerules(self, workspace_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ .clinerules"""
        clinerules_path = workspace_path / ".clinerules"
        
        if not clinerules_path.exists():
            return {
                'exists': False,
                'content': '',
                'rules_count': 0,
                'categories': [],
                'issues': ['–§–∞–π–ª .clinerules –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç']
            }
            
        try:
            content = clinerules_path.read_text(encoding='utf-8')
            rules = self._parse_rules(content)
            
            return {
                'exists': True,
                'content': content,
                'rules_count': len(rules),
                'categories': self._categorize_rules(rules),
                'issues': self._identify_rules_issues(rules),
                'last_modified': clinerules_path.stat().st_mtime
            }
        except Exception as e:
            return {
                'exists': True,
                'content': '',
                'error': str(e),
                'issues': [f'–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}']
            }
            
    def _analyze_workflows(self, workspace_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ workflow —Ñ–∞–π–ª–æ–≤"""
        workflows_dir = workspace_path / ".cline" / "workflows"
        
        if not workflows_dir.exists():
            return {
                'exists': False,
                'workflows': {},
                'count': 0,
                'issues': ['–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è workflows –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç']
            }
            
        workflows = {}
        issues = []
        
        for workflow_file in workflows_dir.glob("*.md"):
            try:
                content = workflow_file.read_text(encoding='utf-8')
                workflows[workflow_file.stem] = {
                    'content': content,
                    'size': len(content),
                    'steps': self._count_workflow_steps(content),
                    'last_modified': workflow_file.stat().st_mtime
                }
            except Exception as e:
                issues.append(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {workflow_file.name}: {e}")
                
        return {
            'exists': True,
            'workflows': workflows,
            'count': len(workflows),
            'issues': issues
        }
        
    def _analyze_vscode_settings(self, vscode_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ VS Code –¥–ª—è Cline"""
        settings_path = vscode_path / "settings.json"
        
        if not settings_path.exists():
            return {
                'exists': False,
                'cline_settings': {},
                'issues': ['–§–∞–π–ª settings.json –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç']
            }
            
        try:
            with open(settings_path, 'r', encoding='utf-8') as f:
                settings = json.load(f)
                
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Cline
            cline_settings = {k: v for k, v in settings.items() if k.startswith('cline.')}
            
            return {
                'exists': True,
                'cline_settings': cline_settings,
                'total_settings': len(settings),
                'issues': self._identify_settings_issues(cline_settings)
            }
        except Exception as e:
            return {
                'exists': True,
                'error': str(e),
                'issues': [f'–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}']
            }
            
    def _analyze_project_structure(self, workspace_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
        structure = {
            'languages': set(),
            'frameworks': set(),
            'file_counts': {},
            'total_files': 0,
            'project_type': 'unknown'
        }
        
        # –ü–æ–¥—Å—á–µ—Ç —Ñ–∞–π–ª–æ–≤ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
        for file_path in workspace_path.rglob('*'):
            if file_path.is_file() and not self._is_ignored_path(file_path):
                structure['total_files'] += 1
                ext = file_path.suffix.lower()
                structure['file_counts'][ext] = structure['file_counts'].get(ext, 0) + 1
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
                if ext in ['.py']:
                    structure['languages'].add('python')
                elif ext in ['.js', '.jsx']:
                    structure['languages'].add('javascript')
                elif ext in ['.ts', '.tsx']:
                    structure['languages'].add('typescript')
                elif ext in ['.java']:
                    structure['languages'].add('java')
                elif ext in ['.cpp', '.c', '.h']:
                    structure['languages'].add('cpp')
                elif ext in ['.rs']:
                    structure['languages'].add('rust')
                elif ext in ['.go']:
                    structure['languages'].add('go')
                    
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –∏ —Ç–∏–ø–∞ –ø—Ä–æ–µ–∫—Ç–∞
        structure['frameworks'] = self._detect_frameworks(workspace_path)
        structure['project_type'] = self._determine_project_type(structure)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è set –≤ list –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        structure['languages'] = list(structure['languages'])
        structure['frameworks'] = list(structure['frameworks'])
        
        return structure
        
    def _analyze_usage_patterns(self, cline_config_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Cline"""
        # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª—ã —Å –∏—Å—Ç–æ—Ä–∏–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        usage_data = {
            'sessions_count': 0,
            'average_session_length': 0,
            'most_used_commands': [],
            'error_patterns': [],
            'common_tasks': []
        }
        
        # –ü–æ–∏—Å–∫ –ª–æ–≥–æ–≤ –∏ –∏—Å—Ç–æ—Ä–∏–∏
        try:
            # –ü–æ–∏—Å–∫ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Cline
            if cline_config_path.exists():
                for log_file in cline_config_path.rglob('*.log'):
                    if log_file.is_file():
                        usage_data = self._parse_usage_logs(log_file, usage_data)
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: {e}")
            
        return usage_data
        
    def _identify_performance_issues(self) -> List[Dict[str, Any]]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é"""
        issues = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
        try:
            import psutil
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
            memory = psutil.virtual_memory()
            if memory.percent > 80:
                issues.append({
                    'type': 'memory',
                    'severity': 'high',
                    'description': f'–í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory.percent}%',
                    'recommendation': '–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏'
                })
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è CPU
            cpu_percent = psutil.cpu_percent(interval=1)
            if cpu_percent > 70:
                issues.append({
                    'type': 'cpu',
                    'severity': 'medium',
                    'description': f'–í—ã—Å–æ–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU: {cpu_percent}%',
                    'recommendation': '–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä'
                })
                
        except ImportError:
            issues.append({
                'type': 'monitoring',
                'severity': 'low',
                'description': '–ú–æ–¥—É–ª—å psutil –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏',
                'recommendation': 'pip install psutil –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞'
            })
            
        return issues
        
    def _identify_optimization_opportunities(self, config_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        opportunities = []
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∞–≤–∏–ª
        if not config_data['current_rules']['exists']:
            opportunities.append({
                'type': 'rules',
                'priority': 'high',
                'description': '–°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∞–≤–∏–ª .clinerules',
                'benefit': '–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞ –∏ —Å–æ–±–ª—é–¥–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤'
            })
        elif config_data['current_rules']['rules_count'] < 10:
            opportunities.append({
                'type': 'rules',
                'priority': 'medium',
                'description': '–†–∞—Å—à–∏—Ä–∏—Ç—å –Ω–∞–±–æ—Ä –ø—Ä–∞–≤–∏–ª –≤ .clinerules',
                'benefit': '–ë–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞'
            })
            
        # –ê–Ω–∞–ª–∏–∑ workflow
        if not config_data['current_workflows']['exists']:
            opportunities.append({
                'type': 'workflows',
                'priority': 'high',
                'description': '–°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ workflow –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞',
                'benefit': '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä—É—Ç–∏–Ω–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏'
            })
            
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        if len(config_data['vscode_settings']['cline_settings']) < 5:
            opportunities.append({
                'type': 'settings',
                'priority': 'medium',
                'description': '–î–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Cline',
                'benefit': '–ë–æ–ª–µ–µ —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è Cline'
            })
            
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞
        project_type = config_data['project_structure']['project_type']
        if project_type != 'unknown':
            opportunities.append({
                'type': 'project_specific',
                'priority': 'high',
                'description': f'–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è {project_type} –ø—Ä–æ–µ–∫—Ç–∞',
                'benefit': '–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∏ workflow –¥–ª—è —Ç–∏–ø–∞ –ø—Ä–æ–µ–∫—Ç–∞'
            })
            
        return opportunities
        
    def _parse_rules(self, content: str) -> List[str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–∞–≤–∏–ª –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
        rules = []
        for line in content.split('\n'):
            line = line.strip()
            if line and not line.startswith('#'):
                rules.append(line)
        return rules
        
    def _categorize_rules(self, rules: List[str]) -> List[str]:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª"""
        categories = set()
        
        for rule in rules:
            rule_lower = rule.lower()
            if any(keyword in rule_lower for keyword in ['test', 'spec', 'unit']):
                categories.add('testing')
            elif any(keyword in rule_lower for keyword in ['security', 'auth', 'password']):
                categories.add('security')
            elif any(keyword in rule_lower for keyword in ['performance', 'optimize', 'cache']):
                categories.add('performance')
            elif any(keyword in rule_lower for keyword in ['format', 'style', 'lint']):
                categories.add('formatting')
            else:
                categories.add('general')
                
        return list(categories)
        
    def _identify_rules_issues(self, rules: List[str]) -> List[str]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –≤ –ø—Ä–∞–≤–∏–ª–∞—Ö"""
        issues = []
        
        if len(rules) == 0:
            issues.append("–§–∞–π–ª –ø—Ä–∞–≤–∏–ª –ø—É—Å—Ç")
        elif len(rules) < 5:
            issues.append("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –ø—Ä–∞–≤–∏–ª –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        if len(rules) != len(set(rules)):
            issues.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø—Ä–∞–≤–∏–ª–∞")
            
        return issues
        
    def _count_workflow_steps(self, content: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —à–∞–≥–æ–≤ –≤ workflow"""
        steps = 0
        for line in content.split('\n'):
            if line.strip().startswith(('1.', '2.', '3.', '-', '*')):
                steps += 1
        return steps
        
    def _identify_settings_issues(self, cline_settings: Dict[str, Any]) -> List[str]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"""
        issues = []
        
        if not cline_settings:
            issues.append("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ Cline –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
            
        return issues
        
    def _is_ignored_path(self, path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—É—Ç—å"""
        ignored_dirs = {'.git', '__pycache__', 'node_modules', '.vscode', 'build', 'dist'}
        return any(part.startswith('.') or part in ignored_dirs for part in path.parts)
        
    def _detect_frameworks(self, workspace_path: Path) -> set:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤"""
        frameworks = set()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if (workspace_path / "package.json").exists():
            frameworks.add("nodejs")
            
        if (workspace_path / "requirements.txt").exists() or (workspace_path / "pyproject.toml").exists():
            frameworks.add("python")
            
        if (workspace_path / "Cargo.toml").exists():
            frameworks.add("rust")
            
        if (workspace_path / "go.mod").exists():
            frameworks.add("go")
            
        return frameworks
        
    def _determine_project_type(self, structure: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
        languages = structure['languages']
        frameworks = structure['frameworks']
        
        if 'python' in languages:
            return 'python'
        elif 'javascript' in languages or 'typescript' in languages:
            return 'web'
        elif 'java' in languages:
            return 'java'
        elif 'rust' in languages:
            return 'rust'
        elif 'go' in languages:
            return 'go'
        elif 'cpp' in languages:
            return 'cpp'
        else:
            return 'unknown'
            
    def _parse_usage_logs(self, log_file: Path, usage_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        try:
            content = log_file.read_text(encoding='utf-8')
            # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
            lines = content.split('\n')
            usage_data['sessions_count'] += content.count('session start')
            
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–æ–≥–∞ {log_file}: {e}")
            
        return usage_data
